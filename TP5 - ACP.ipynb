{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Matrice-de-covariance\" data-toc-modified-id=\"Matrice-de-covariance-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Matrice de covariance</a></span></li><li><span><a href=\"#Calcul-des-composantes\" data-toc-modified-id=\"Calcul-des-composantes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calcul des composantes</a></span></li><li><span><a href=\"#Dataset-Iris\" data-toc-modified-id=\"Dataset-Iris-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Dataset <em>Iris</em></a></span></li><li><span><a href=\"#Dataset-MNIST\" data-toc-modified-id=\"Dataset-MNIST-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset MNIST</a></span></li><li><span><a href=\"#Complément:-partitionnement-des-données\" data-toc-modified-id=\"Complément:-partitionnement-des-données-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Complément: partitionnement des données</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP n°5 &nbsp;-&nbsp; analyse en composantes principales\n",
    "\n",
    "<br>\n",
    "\n",
    "L'analyse en composante principales est une méthode d'apprentissage non supervisé dont l'objectif est généralement de réduire la dimension des données. Au cours de cette séance nous allons revenir sur les principes de cette méthode puis nous verrons comment elle peut être appliquée sur différents jeux de données.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de covariance\n",
    "\n",
    "<br>\n",
    "\n",
    "On considère un ensemble de $N$ données $\\mathcal{D} = \\{x^{(i)}\\}_{1 \\le i \\le N}$ avec $x^{(i)} \\in \\mathbb{R}^d$. Les directions des composantes principales sont les vecteurs propres de la matrice de covariance associée à cet ensemble de données. Dans cette première partie vous allez calculer la matrice de covariance pour différents ensembles de données 2D artificielles.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Créez un ensemble de $N$ = 10$^6$ points en dimension 2 tels que les 2 composantes de chaque point sont tirées aléatoirement entre 0 et 1. Les données seront mises sous la forme d'une matrice $X$ de dimension $N \\times 2$.\n",
    "- Calculez la matrice de covariance $S = \\frac{1}{N}\\,X^TX$.\n",
    "- La valeur des différents coefficients est-elle conforme à ce que l'on attend ? Si ce n'est pas le cas, quel est le problème ?\n",
    "- Corrigez le problème afin de calculer correctement la matrice $S$.\n",
    "- Comparez avec le résultat renvoyés par les fonctions <code>numpy.cov(X)</code>.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Pour étudier les corrélations entre différentes variables, on s'appuie le plus souvent sur la **matrice de corrélation**. Le coefficient de corrélation de 2 variables $x$ et $y$ est défini par\n",
    "$$ \\rho(x,y) \\;=\\; \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y} $$\n",
    "Pour calculer simplement la matrice de corrélation, on peut modifier la matrice des données $X$ afin que chaque variable ait une variance égale à 1.\n",
    "\n",
    "- Calculer la matrice de corrélation associées aux données précédemment créées.\n",
    "- Comparer avec le résultat renvoyé par la fonction <code>numpy.corrcoef()</code>.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Refaire l'exercice précédent dans le cas de données où les 2 composantes sont linéairement reliées.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Calcul des composantes\n",
    "\n",
    "<br>\n",
    "\n",
    "Les directions des composantes principales peuvent être obtenues soit en diagonalisant la matrice de covariance, soit en effectuant une décomposition en valeurs singulières de la matrice des données $X$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Construire la matrice $X$ pour un ensemble de $N$=10000 données 2D telles que la seconde composante est de la forme\n",
    "$$ x_2^{(i)} \\;=\\; \\alpha\\,x_1^{(i)} \\;+\\; \\varepsilon^{(i)} $$\n",
    "où $\\varepsilon$ est une variable aléatoire gaussienne de moyenne nulle est de variance $\\ll$ 1. On centrera les données et on les normalisera de façon à ce que les 2 composantes aient une variance unitaire.\n",
    "- Calculez la matrice de covariance (ici identique à la matrice de corrélation).\n",
    "- Représentez une centaine de points de ces données sur un graphe dans le plan $(x_1,x_2)$.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- A l'aide de la fonction <code>numpy.linalg.eigh()</code>, calculez les valeurs propres et les vecteurs propres de la matrice de covariance.\n",
    "- A l'aide de la fonction <code>numpy.linalg.svd()</code>, calculez les valeurs singulières de $X$ ainsi que les directions des composantes principales.\n",
    "- Quelle relation a t-on entre les valeurs propres et les valeurs singulières ? Cette relation est-elle bien vérifiée ?\n",
    "- Vérifiez que les vecteurs propres de la matrice de covariance forment bien une base orthonormée.\n",
    "- Représentez sur un graphe une centaine de points des données ainsi que les directions correspondant aux composantes principales.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Projetez les données sur les directions des composantes principales $\\rightarrow$ matrice $Z$.\n",
    "- Faire une copie de $Z$ sur laquelle vous annulerez les coefficients correspondant à la seconde composante $\\rightarrow$ matrice $Z_{modif}$.\n",
    "- Exprimez la matrice $Z_{modif}$ dans la base initiale $\\rightarrow$ matrice $X_{modif}$.\n",
    "- Représentez sur 4 graphes les données $X$, $Z$, $Z_{modif}$ et $X_{modif}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice** \n",
    "\n",
    "La méthode d'analyse en composantes principales est implémentée dans la bibliothèque scikit-learn (<code>sklearn.decomposition.PCA()</code>). \n",
    "\n",
    "- A quoi correspond l'argument n_components qui peut être fourni lors de l'appel de cette fonction ?\n",
    "- Vérifiez que les composantes principales renvoyées par cette fonction correspondent bien à celles que vous avez calculées précédemment.\n",
    "- Utilisez les méthodes <code>.transform()</code> et <code>.inverse_transform()</code> pour réaliser le même traitement des données que celui effectué précédemment.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dataset *Iris*\n",
    "\n",
    "<br>\n",
    "\n",
    "Nous allons maintenant utiliser l'analyse en composantes principales sur le dataset *Iris*. La dimension des données étant relativement petite ($d=4$), on va pouvoir visualiser facilement les principales propriétés statistiques des ces données en utilisant la fonction <code>pairplot()</code> de la librairie *seaborn*. Pour pouvoir utiliser cette fonction, les données doivent être importées sous la forme d'un dataframe *pandas*. On va donc tout d'abord charger ces deux librairies avant d'importer les données du dataset.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Importer les données du dataset *Iris* sous la forme de 2 matrices X et y au format dataframe de la librairie pandas (voir documentation sur scikit-learn).\n",
    "- La librairie pandas offre une large gamme d'outils pour la manipulation et l'analyse de données. A titre d'exemple vous pouvez appliquer la méthode <code>.describe()</code> sur ces deux variables afin d'afficher leurs propriétés statistiques. Quelles sont les informations renvoyées ?\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "La librairie *seaborn* contient un ensemble d'outils destinés à la visualisation de données statistiques. Lorsque la dimension des données n'est pas trop grande, la méthode *pairplot* fournit une représentation intéressante de la distribution et des corrélations entre les différentes variables.\n",
    "\n",
    "- Utilisez cette fonction pour représenter les données de la matrice X.\n",
    "- Que peut-on observer ?\n",
    "- Ces observations sont-elles en accord avec la valeur des coefficients de la matrice de corrélation ?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Effectuez une analyse en composantes principales sur les données $X$ du dataset *Iris*.\n",
    "- Déterminez la proportion de variance expliquée par chacune des composantes et représentez le résultat sur un graphe (<code>matplotlib.pyplot.bar()</code>).\n",
    "- Projetez les données sur la base des composantes principales et représentez les dans le plan (composante 1, composante 2) en attribuant une couleur pour chacun des espèces.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Dataset MNIST\n",
    "\n",
    "<br>\n",
    "\n",
    "Dans cette partie nous allons utiliser les données MNIST afin de regarder comment l'analyse en composantes principales permet de réduire la dimension des données.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Variance expliquée\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Sélectionnez un sous-ensemble comportant 10000 données afin de réduire les temps de calcul puis partitionnez le en 1 ensemble d'entraînement et 1 ensemble de test.\n",
    "- Effectuez une analyse en composantes principales sur les données d'entraînement.\n",
    "- Représentez sur un graphe la variance totale expliquée en fonction du nombre de composantes conservées.\n",
    "- Combien de composantes permettent d'expliquer 90% de la variance des données ? Et pour 99% ?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation des données\n",
    "\n",
    "<br>\n",
    "\n",
    "Pour un ensemble de données $\\{x^{(i)}\\}_{1 \\le i \\le n}$ de dimension $d$, l'ACP produit une base orthonormée $\\{v_j\\}_{1 \\le j \\le d}$ de $\\mathbb{R}^d$ et on peut donc écrire une donnée $x$ quelconque sous la forme\n",
    "\n",
    "$$ x \\;=\\; \\sum_{j=1}^d \\, \\varphi_j\\,v_j $$\n",
    "\n",
    "En tronquant cette somme, on produit une approximation $\\tilde{x}$ de $x$:\n",
    "\n",
    "$$ \\tilde{x} \\;=\\; \\sum_{j=1}^m \\, \\varphi_j\\,v_j \\;\\;\\; \\text{avec} \\;\\;\\; m < d $$\n",
    "\n",
    "On va maintenant montrer sur un exemple que plus on ajoute de composantes dans cette somme, meilleure est l'approximation obtenue.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "- Sélectionner une donnée du dataset MNIST et l'afficher sous forme d'une image.\n",
    "- Afficher sur une série de graphes les images obtenues lorsqu'on conserve les $n$ premières composantes, pour $n$ = 10, 20, 40, 80, 160, 320 et 640.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Compléments\n",
    "\n",
    "<br>\n",
    "\n",
    "Représentez sur un graphe l'évolution du score de test obtenu par régression logistique sur les données MNIST en fonction du nombre de composantes conservées. Afin de ne pas trop allonger la durée du calcul, vous pourrez faire varier le nombre de composantes conservées par pas de 1 entre 1 et 10, puis par pas de 5 entre 10 et 100 et enfin par pas de 10 entre 100 et 400.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
